name: URC 2026 Testing Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run comprehensive tests daily at 2 AM UTC
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.10'
  ROS_DISTRO: humble

jobs:
  # Phase 1: Import Path Validation
  import-validation:
    name: Import Path Validation
    runs-on: ubuntu-latest
    container: ros:humble
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
      
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        apt-get update
        apt-get install -y python3-pip
        pip install -e .
        
    - name: Run import fixes
      run: |
        python tools/scripts/phase1_import_fixes.py
        
    - name: Validate imports
      run: |
        python -c "
        import sys
        sys.path.insert(0, 'src')
        try:
            from autonomy.control.integrated_critical_systems import IntegratedCriticalSystems
            from simulation.hil.hil_manager import HILManager
            print('‚úÖ All critical imports successful')
        except ImportError as e:
            print(f'‚ùå Import validation failed: {e}')
            sys.exit(1)
        "

  # Phase 2: HIL Testing
  hil-testing:
    name: Hardware-in-the-Loop Testing
    runs-on: ubuntu-latest
    needs: import-validation
    container: ros:humble
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
      
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install HIL dependencies
      run: |
        apt-get update
        apt-get install -y python3-pip python3-serial
        pip install pyserial psutil scapy websockets pytest-asyncio
        
    - name: Setup HIL environment
      run: |
        python tools/scripts/phase2_hil_infrastructure.py
        
    - name: Run HIL tests
      run: |
        python -c "
        import sys
        sys.path.insert(0, 'simulation')
        from hil.hil_test_framework import create_hil_test_framework
        
        framework = create_hil_test_framework()
        results = framework.run_all_tests()
        
        # Check results
        failed = len([r for r in results if r.result.value != 'pass'])
        if failed > 0:
            print(f'‚ùå {failed} HIL tests failed')
            sys.exit(1)
        else:
            print('‚úÖ All HIL tests passed')
        "
        
    - name: Upload HIL test reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: hil-test-reports
        path: hil_test_report_*.json

  # Phase 3: WebSocket Network Testing
  websocket-testing:
    name: WebSocket Network Testing
    runs-on: ubuntu-latest
    needs: import-validation
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
      
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install WebSocket dependencies
      run: |
        pip install websockets aiohttp pytest-asyncio psutil scapy
        
    - name: Setup WebSocket testing
      run: |
        python tools/scripts/phase3_websocket_testing.py
        
    - name: Start WebSocket test server
      run: |
        python tests/network/websocket_test_server.py &
        sleep 5  # Give server time to start
        
    - name: Run WebSocket tests
      run: |
        python -c "
        import asyncio
        import sys
        sys.path.insert(0, '.')
        
        from tests.network.websocket_network_tester import create_websocket_tester, NetworkTestConfig, TestType
        
        async def run_tests():
            tester = create_websocket_tester()
            
            # Test connection reliability
            config = NetworkTestConfig(
                test_type=TestType.CONNECTION_RELIABILITY,
                target_host='localhost',
                target_port=8080,
                duration=10.0,
                client_count=2
            )
            
            result = await tester.run_connection_reliability_test(config)
            
            if result.success:
                print('‚úÖ WebSocket connection test passed')
            else:
                print('‚ùå WebSocket connection test failed')
                print(f'Errors: {result.error_details}')
                return False
                
            return True
        
        success = asyncio.run(run_tests())
        if not success:
            sys.exit(1)
        "
        
    - name: Upload WebSocket test reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: websocket-test-reports
        path: websocket_test_report_*.json

  # Phase 4: Performance Testing
  performance-testing:
    name: Performance Degradation Testing
    runs-on: ubuntu-latest
    needs: import-validation
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
      
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install performance dependencies
      run: |
        pip install psutil memory-profiler pytest-benchmark
        
    - name: Setup performance testing
      run: |
        python tools/scripts/phase4_performance_testing.py
        
    - name: Run performance tests
      run: |
        python -c "
        import sys
        sys.path.insert(0, '.')
        
        from tests.performance.performance_degradation_tester import create_performance_tester
        
        # Create tester
        tester = create_performance_tester()
        
        # Establish baseline (shortened for CI)
        print('Establishing performance baseline...')
        baseline = tester.establish_baseline(duration=10.0)
        
        # Run quick performance test
        from tests.performance.performance_degradation_tester import PerformanceTest, TestPattern, ResourceType
        
        test = PerformanceTest(
            name='ci_performance_test',
            test_pattern=TestPattern.GRADUAL_LOAD,
            target_resource=ResourceType.CPU,
            duration=15.0,
            intensity=0.5
        )
        
        result = tester.run_performance_test(test)
        
        if result.success and not result.degradation_detected:
            print('‚úÖ Performance test passed')
        else:
            print('‚ö†Ô∏è Performance test detected degradation')
            print(f'Degradation: {result.degradation_percentage:.1f}%')
            # Don't fail CI for performance, just warn
        "
        
    - name: Upload performance reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-test-reports
        path: performance_test_report_*.json

  # Integration Testing
  integration-testing:
    name: Full System Integration Testing
    runs-on: ubuntu-latest
    needs: [hil-testing, websocket-testing, performance-testing]
    container: ros:humble
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
      
    - name: Setup ROS2
      run: |
        source /opt/ros/humble/setup.bash
        
    - name: Build ROS2 packages
      run: |
        source /opt/ros/humble/setup.bash
        colcon build --packages-select autonomy
        
    - name: Run integration tests
      run: |
        source /opt/ros/humble/setup.bash
        source install/setup.bash
        
        # Run test suite
        python -m pytest tests/integration/ -v --tb=short
        
    - name: Run system validation
      run: |
        python scripts/validate_critical_systems.py

  # Security and Code Quality
  code-quality:
    name: Code Quality & Security
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
      
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install linting tools
      run: |
        pip install flake8 black isort mypy bandit safety
        
    - name: Run code formatting check
      run: |
        black --check src/ tests/ simulation/ || true
        
    - name: Run import sorting check
      run: |
        isort --check-only src/ tests/ simulation/ || true
        
    - name: Run linting
      run: |
        flake8 src/ tests/ simulation/ --max-line-length=100 --ignore=E203,W503 || true
        
    - name: Run type checking
      run: |
        mypy src/ --ignore-missing-imports || true  # Don't fail CI for type errors
        
    - name: Run security scan
      run: |
        bandit -r src/ -f json -o bandit-report.json || true
        safety check --json --output safety-report.json || true
        
    - name: Upload security reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-reports
        path: bandit-report.json safety-report.json

  # Test Reporting and Analysis
  test-reporting:
    name: Test Reporting & Analysis
    runs-on: ubuntu-latest
    needs: [import-validation, hil-testing, websocket-testing, performance-testing, integration-testing, code-quality]
    if: always()
    
    steps:
    - name: Download all test artifacts
      uses: actions/download-artifact@v3
      
    - name: Generate comprehensive test report
      run: |
        python -c "
        import json
        import os
        from pathlib import Path
        
        # Collect all test reports
        reports = {}
        
        # HIL reports
        for hil_file in Path('.').glob('hil-test-reports/hil_test_report_*.json'):
            with open(hil_file) as f:
                reports['hil'] = json.load(f)
        
        # WebSocket reports
        for ws_file in Path('.').glob('websocket-test-reports/websocket_test_report_*.json'):
            with open(ws_file) as f:
                reports['websocket'] = json.load(f)
        
        # Performance reports
        for perf_file in Path('.').glob('performance-test-reports/performance_test_report_*.json'):
            with open(perf_file) as f:
                reports['performance'] = json.load(f)
        
        # Generate summary
        summary = {
            'timestamp': os.environ.get('GITHUB_RUN_TIME', 'unknown'),
            'commit': os.environ.get('GITHUB_SHA', 'unknown'),
            'branch': os.environ.get('GITHUB_REF_NAME', 'unknown'),
            'reports': reports
        }
        
        with open('comprehensive-test-report.json', 'w') as f:
            json.dump(summary, f, indent=2)
        
        print('‚úÖ Comprehensive test report generated')
        "
        
    - name: Upload comprehensive report
      uses: actions/upload-artifact@v3
      with:
        name: comprehensive-test-report
        path: comprehensive-test-report.json
        
    - name: Comment PR with results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          
          try {
            const report = JSON.parse(fs.readFileSync('comprehensive-test-report.json', 'utf8'));
            
            let comment = '## üß™ Test Results Summary\\n\\n';
            
            // HIL results
            if (report.reports.hil) {
              const hil = report.reports.hil.summary;
              comment += `**HIL Testing:** ${hil.passed}/${hil.total_tests} passed (${hil.success_rate.toFixed(1)}%)\\n`;
            }
            
            // WebSocket results
            if (report.reports.websocket) {
              const ws = report.reports.websocket.summary;
              comment += `**WebSocket Testing:** ${ws.successful_tests}/${ws.total_tests} passed (${ws.success_rate.toFixed(1)}%)\\n`;
            }
            
            // Performance results
            if (report.reports.performance) {
              const perf = report.reports.performance.summary;
              comment += `**Performance Testing:** ${perf.successful_tests}/${perf.total_tests} passed\\n`;
            }
            
            comment += `\\nüìä [Detailed Report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          } catch (error) {
            console.log('Could not generate PR comment:', error.message);
          }

  # Deployment (only on main branch success)
  deploy:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [integration-testing, code-quality]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
      
    - name: Deploy to staging
      run: |
        echo "üöÄ Deploying to staging environment..."
        # Add deployment commands here
        
    - name: Run smoke tests
      run: |
        echo "üî• Running smoke tests..."
        # Add smoke test commands here
        
    - name: Deploy to production
      if: success()
      run: |
        echo "üéØ Deploying to production environment..."
        # Add production deployment commands here