name: URC 2026 Competition CI/CD

# Disabled on push/PR - run manually via Actions tab if needed
on:
  workflow_dispatch:

jobs:
  test-validation:
    name: Test Validation & Quality Gates
    runs-on: ubuntu-22.04
    container:
      image: ubuntu:22.04

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install system dependencies
        run: |
          apt-get update && apt-get install -y \
            python3 \
            python3-pip \
            python3-dev \
            git \
            curl \
            wget \
            lsb-release \
            gnupg \
            software-properties-common

      - name: Setup ROS2 Humble
        run: |
          curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key -o /usr/share/keyrings/ros-archive-keyring.gpg
          echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $(lsb_release -cs) main" | tee /etc/apt/sources.list.d/ros2.list > /dev/null
          apt-get update && apt-get install -y \
            ros-humble-ros-base \
            ros-humble-ament-cmake \
            python3-colcon-common-extensions \
            python3-rosdep

      - name: Initialize rosdep
        run: |
          rosdep init || true
          rosdep update

      - name: Install Python dependencies
        run: |
          python3 -m pip install --upgrade pip
          pip3 install \
            pytest \
            pytest-benchmark \
            pytest-cov \
            pytest-xdist \
            pytest-html \
            pyyaml \
            numpy \
            websockets \
            simple-websocket

      - name: Source ROS2 environment
        run: |
          source /opt/ros/humble/setup.bash
          echo "ROS_DISTRO=humble" >> $GITHUB_ENV

      - name: Create test results directory
        run: mkdir -p test-results

      # ===== QUALITY GATES =====

      - name: Run Code Quality Checks
        run: |
          source /opt/ros/humble/setup.bash
          python3 -m pytest tests/unit/ --tb=short --junitxml=test-results/unit-tests.xml --html=test-results/unit-report.html --self-contained-html

      - name: Run Competition Critical Tests
        run: |
          source /opt/ros/humble/setup.bash
          python3 -m pytest tests/competition/ --tb=short --junitxml=test-results/competition-tests.xml --html=test-results/competition-report.html --self-contained-html

      - name: Run Integration Tests
        run: |
          source /opt/ros/humble/setup.bash
          python3 -m pytest tests/integration/ --tb=short --junitxml=test-results/integration-tests.xml --html=test-results/integration-report.html --self-contained-html

      - name: Run Performance Benchmarks
        run: |
          source /opt/ros/humble/setup.bash
          python3 -m pytest src/autonomy/core/state_management/tests/test_adaptive_performance.py --benchmark-only --benchmark-json=test-results/performance.json

      - name: Validate Performance Standards
        run: |
          python3 -c "
          import json
          import sys

          # Load performance results
          with open('test-results/performance.json', 'r') as f:
              data = json.load(f)

          # Check performance standards
          standards = {
              'test_context_evaluation_performance': {'max_time': 0.1, 'max_variability': 0.5},
              'test_policy_engine_performance': {'max_time': 0.05, 'max_variability': 0.5}
          }

          violations = []
          for benchmark in data['benchmarks']:
              test_name = benchmark['name'].split('::')[-1]
              if test_name in standards:
                  std = standards[test_name]
                  mean_time = benchmark['stats']['mean']
                  variability = benchmark['stats']['stddev'] / benchmark['stats']['mean'] if benchmark['stats']['mean'] > 0 else 0

                  if mean_time > std['max_time']:
                      violations.append(f'{test_name}: mean time {mean_time:.3f}s exceeds {std[\"max_time\"]}s')

                  if variability > std['max_variability']:
                      violations.append(f'{test_name}: variability {variability:.2f} exceeds {std[\"max_variability\"]}')

          if violations:
              print('PERFORMANCE STANDARD VIOLATIONS:')
              for v in violations:
                  print(f'  âŒ {v}')
              sys.exit(1)
          else:
              print('âœ… All performance standards met')
          "

      - name: Generate Test Quality Report
        run: |
          python3 -c "
          import xml.etree.ElementTree as ET
          import json

          # Parse test results
          results = {}
          for test_type in ['unit', 'competition', 'integration']:
              try:
                  tree = ET.parse(f'test-results/{test_type}-tests.xml')
                  root = tree.getroot()
                  testsuite = root.find('testsuite')
                  if testsuite is not None:
                      results[test_type] = {
                          'tests': int(testsuite.get('tests', 0)),
                          'failures': int(testsuite.get('failures', 0)),
                          'errors': int(testsuite.get('errors', 0)),
                          'skipped': int(testsuite.get('skipped', 0))
                      }
              except:
                  results[test_type] = {'tests': 0, 'failures': 0, 'errors': 0, 'skipped': 0}

          # Calculate quality metrics
          total_tests = sum(r['tests'] for r in results.values())
          total_failures = sum(r['failures'] for r in results.values())
          total_errors = sum(r['errors'] for r in results.values())
          pass_rate = ((total_tests - total_failures - total_errors) / total_tests * 100) if total_tests > 0 else 0

          # Competition readiness check
          competition_passed = results['competition']['failures'] == 0 and results['competition']['errors'] == 0
          integration_passed = results['integration']['failures'] == 0 and results['integration']['errors'] == 0

          print('=== URC 2026 TEST QUALITY REPORT ===')
          print(f'Total Tests: {total_tests}')
          print(f'Pass Rate: {pass_rate:.1f}%')
          print(f'Failures: {total_failures}, Errors: {total_errors}')
          print()
          print('By Category:')
          for category, stats in results.items():
              cat_pass_rate = ((stats['tests'] - stats['failures'] - stats['errors']) / stats['tests'] * 100) if stats['tests'] > 0 else 0
              print(f'  {category.title()}: {stats[\"tests\"]} tests, {cat_pass_rate:.1f}% pass')

          print()
          print('Competition Readiness:')
          print(f'  Critical Tests: {\"âœ… PASS\" if competition_passed else \"âŒ FAIL\"}')
          print(f'  Integration Tests: {\"âœ… PASS\" if integration_passed else \"âŒ FAIL\"}')
          print(f'  Overall Readiness: {\"âœ… COMPETITION READY\" if (competition_passed and integration_passed and pass_rate >= 85) else \"âŒ NOT READY\"}')

          # Write to file
          with open('test-results/quality-report.json', 'w') as f:
              json.dump({
                  'summary': {
                      'total_tests': total_tests,
                      'pass_rate': pass_rate,
                      'competition_ready': competition_passed and integration_passed and pass_rate >= 85
                  },
                  'categories': results
              }, f, indent=2)
          "

      # ===== COMPETITION READINESS GATES =====

      - name: Competition Readiness Gate
        run: |
          # Read quality report
          if [ -f test-results/quality-report.json ]; then
            COMPETITION_READY=$(python3 -c "
            import json
            with open('test-results/quality-report.json', 'r') as f:
                data = json.load(f)
            print('true' if data['summary']['competition_ready'] else 'false')
            ")

            if [ \"$COMPETITION_READY\" = \"true\" ]; then
              echo \"ðŸŽ‰ COMPETITION READINESS: PASSED\"
              echo \"ðŸš€ System is ready for URC 2026 competition deployment\"
            else
              echo \"âš ï¸ COMPETITION READINESS: FAILED\"
              echo \"âŒ System requires additional testing before competition\"
              exit 1
            fi
          else
            echo \"âŒ Quality report not generated\"
            exit 1
          fi

      # ===== ARTIFACTS =====

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-${{ github.run_number }}
          path: test-results/

      - name: Upload performance results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-results-${{ github.run_number }}
          path: test-results/performance.json

  deployment-validation:
    name: Deployment Validation
    runs-on: ubuntu-22.04
    needs: test-validation
    if: github.ref == 'refs/heads/main'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Validate deployment configuration
        run: |
          python3 -c "
          import yaml
          import sys

          # Check if config files exist
          config_files = ['config/rover.yaml', 'config/competition.yaml']
          for config_file in config_files:
              try:
                  with open(config_file, 'r') as f:
                      config = yaml.safe_load(f)
                  print(f'âœ… {config_file}: Valid')
              except Exception as e:
                  print(f'âŒ {config_file}: Invalid - {e}')
                  sys.exit(1)

          print('âœ… All configuration files validated')
          "

      - name: Check deployment scripts
        run: |
          # Verify deployment scripts exist and are executable
          scripts=(
            'scripts/deployment/automated_deployment.py'
            'scripts/competition/pre_competition_checklist.py'
            'scripts/competition/competition_telemetry.py'
            'scripts/monitoring/service_health_monitor.py'
          )

          for script in \"${scripts[@]}\"; do
            if [ -f \"$script\" ]; then
              echo \"âœ… $script: Exists\"
            else
              echo \"âŒ $script: Missing\"
              exit 1
            fi
          done

          echo \"âœ… All deployment scripts present\"

  performance-monitoring:
    name: Performance Monitoring
    runs-on: ubuntu-22.04
    if: github.event_name == 'push'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup performance monitoring
        run: |
          pip install pytest-benchmark[histogram]

      - name: Run performance regression tests
        run: |
          python3 -m pytest src/autonomy/core/state_management/tests/test_adaptive_performance.py \
            --benchmark-only \
            --benchmark-compare \
            --benchmark-compare-fail=0.05 \
            --benchmark-json=test-results/perf-regression.json || true

      - name: Performance regression analysis
        run: |
          python3 -c "
          import json
          import os

          regression_file = 'test-results/perf-regression.json'
          if os.path.exists(regression_file):
              with open(regression_file, 'r') as f:
                  data = json.load(f)

              print('Performance Regression Analysis:')
              for benchmark in data.get('benchmarks', []):
                  name = benchmark['name']
                  change = benchmark.get('change', 0)
                  if abs(change) > 0.05:  # 5% change threshold
                      status = 'âš ï¸ REGRESSION' if change > 0 else 'âœ… IMPROVEMENT'
                      print(f'  {status} {name}: {change:.1%} change')
                  else:
                      print(f'  âœ… STABLE {name}: {change:.1%} change')
          else:
              print('No previous performance data for comparison')
          "
