#!/usr/bin/env python3
"""
Comprehensive System Test Runner - URC 2026

Runs full system integration tests with comprehensive monitoring:
- Behavior trees execution and monitoring
- State machines transitions
- Network connectivity tests
- Simulator integration
- Performance metrics collection
- Real-time dashboard updates

This test suite validates the complete URC 2026 rover system.

Author: URC 2026 System Integration Team
"""

import time
import threading
import subprocess
import sys
from pathlib import Path
from typing import Dict, List, Any

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from tests.performance.testing_metrics_collector import TestMetricsCollector
from tests.performance.system_integration_monitor import system_monitor
from tests.performance.testing_metrics_dashboard import run_server
import psutil


class ComprehensiveSystemTester:
    """Runs comprehensive system integration tests."""

    def __init__(self):
        self.metrics_collector = TestMetricsCollector()
        self.test_results = {}
        self.test_duration = 120  # 2 minutes

    def run_comprehensive_system_test(self) -> Dict[str, Any]:
        """Run comprehensive system integration test."""
        print("ðŸš€ Starting Comprehensive System Integration Test")
        print("=" * 60)
        print(f"Test Duration: {self.test_duration} seconds")
        print()

        start_time = time.time()

        try:
            # Start metrics collection and system monitoring
            print("ðŸ“Š Starting metrics collection and system monitoring...")
            self.metrics_collector.start_collection()

            # Simulate system startup
            self._simulate_system_startup()

            # Run test phases
            self._run_behavior_tree_tests()
            self._run_state_machine_tests()
            self._run_network_tests()
            self._run_performance_tests()
            self._run_simulator_integration_tests()

            # Wait for monitoring to collect data
            print(f"â±ï¸  Running system monitoring for {self.test_duration} seconds...")
            time.sleep(self.test_duration)

            # Generate comprehensive report
            report = self._generate_system_test_report(start_time)

            return report

        finally:
            # Stop monitoring
            self.metrics_collector.stop_collection()
            print("\nðŸ›‘ System monitoring stopped")

    def _simulate_system_startup(self):
        """Simulate system startup sequence."""
        print("ðŸ”„ Simulating system startup...")

        # Simulate behavior tree startup
        system_monitor.simulate_test_data()

        # Add some realistic test data
        for i in range(3):
            self.metrics_collector.collect_performance_metric(
                'startup', f'initialization_step_{i}', time.time() % 100
            )

        print("âœ… System startup simulation complete")

    def _run_behavior_tree_tests(self):
        """Run behavior tree integration tests."""
        print("ðŸŒ³ Testing Behavior Tree Integration...")

        # Simulate BT execution
        bt_scenarios = [
            {'name': 'waypoint_navigation', 'duration': 5.0, 'success': True},
            {'name': 'obstacle_avoidance', 'duration': 3.0, 'success': True},
            {'name': 'sample_collection', 'duration': 7.0, 'success': False},  # Simulate failure
        ]

        for scenario in bt_scenarios:
            start_time = time.time()

            # Simulate BT execution
            time.sleep(scenario['duration'] * 0.1)  # Fast simulation

            duration = time.time() - start_time
            success = scenario['success']

            # Record metrics
            self.metrics_collector.collect_performance_metric(
                'behavior_trees', f'{scenario["name"]}_duration', duration
            )
            self.metrics_collector.collect_performance_metric(
                'behavior_trees', f'{scenario["name"]}_success', 1 if success else 0
            )

            print(f"  {'âœ…' if success else 'âŒ'} {scenario['name']}: {duration:.2f}s")

    def _run_state_machine_tests(self):
        """Run state machine integration tests."""
        print("ðŸ”„ Testing State Machine Integration...")

        # Simulate state transitions
        states = ['IDLE', 'INITIALIZING', 'NAVIGATING', 'SAMPLING', 'RETURNING']
        transitions = []

        current_state = 'IDLE'
        for target_state in states[1:]:
            start_time = time.time()

            # Simulate transition
            time.sleep(0.5)  # Transition time

            transition_time = time.time() - start_time
            valid_transition = True  # Assume valid

            transitions.append({
                'from': current_state,
                'to': target_state,
                'duration': transition_time,
                'valid': valid_transition
            })

            self.metrics_collector.collect_performance_metric(
                'state_machines', f'transition_{current_state}_to_{target_state}', transition_time
            )

            current_state = target_state
            print(f"  ðŸ”„ {current_state} â†’ {target_state}: {transition_time:.3f}s")

    def _run_network_tests(self):
        """Run network connectivity tests."""
        print("ðŸŒ Testing Network Connectivity...")

        # Test different network scenarios
        network_tests = [
            {'interface': 'wifi_main', 'latency_ms': 5.2, 'connected': True},
            {'interface': 'lte_backup', 'latency_ms': 45.8, 'connected': True},
            {'interface': 'ethernet_direct', 'latency_ms': 0.8, 'connected': False},  # Disconnected
        ]

        for test in network_tests:
            # Simulate network test
            time.sleep(0.2)

            self.metrics_collector.collect_performance_metric(
                'network', f'{test["interface"]}_latency', test['latency_ms']
            )
            self.metrics_collector.collect_performance_metric(
                'network', f'{test["interface"]}_connected', 1 if test['connected'] else 0
            )

            status = "âœ…" if test['connected'] else "âŒ"
            print(f"  {status} {test['interface']}: {test['latency_ms']:.1f}ms")

    def _run_performance_tests(self):
        """Run performance benchmark tests."""
        print("âš¡ Running Performance Benchmarks...")

        # Import and run performance tests
        try:
            from tests.performance.test_performance_baseline import PerformanceBaselineTester

            tester = PerformanceBaselineTester()
            results = tester.run_all_baselines()

            # Record key metrics
            if 'results' in results:
                for test_name, test_data in results['results'].items():
                    if isinstance(test_data, dict) and 'p99_ms' in test_data:
                        self.metrics_collector.collect_performance_metric(
                            'performance', f'{test_name}_p99', test_data['p99_ms']
                        )
                        print(".3f")

        except Exception as e:
            print(f"  âš ï¸ Performance test error: {e}")

    def _run_simulator_integration_tests(self):
        """Run simulator integration tests."""
        print("ðŸŽ® Testing Simulator Integration...")

        # Check for running simulator processes
        sim_found = False
        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
            try:
                name = proc.info['name'].lower()
                cmdline = ' '.join(proc.info['cmdline'] or []).lower()
                if 'gazebo' in name or 'gzserver' in name or 'simulator' in cmdline:
                    sim_found = True
                    self.metrics_collector.collect_performance_metric(
                        'simulator', 'process_found', 1
                    )
                    self.metrics_collector.collect_performance_metric(
                        'simulator', 'process_cpu', proc.cpu_percent()
                    )
                    break
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                continue

        if sim_found:
            print("  âœ… Simulator process detected and monitored")
        else:
            print("  âš ï¸ No simulator processes detected (expected for integration test)")

        # Simulate simulator metrics
        self.metrics_collector.collect_performance_metric(
            'simulator', 'real_time_factor', 0.95
        )
        self.metrics_collector.collect_performance_metric(
            'simulator', 'physics_uptime', time.time() - time.time() + 120  # 2 minutes
        )

    def _generate_system_test_report(self, start_time: float) -> Dict[str, Any]:
        """Generate comprehensive system test report."""
        print("ðŸ“‹ Generating Comprehensive System Test Report...")

        end_time = time.time()
        total_duration = end_time - start_time

        # Get final metrics
        dashboard_data = self.metrics_collector.metrics_store.get_dashboard_data()

        report = {
            'test_metadata': {
                'start_time': start_time,
                'end_time': end_time,
                'duration_seconds': total_duration,
                'test_components': [
                    'behavior_trees', 'state_machines', 'network',
                    'performance', 'simulator', 'system_integration'
                ]
            },
            'system_integration_status': dashboard_data.get('system_integration', {}),
            'performance_metrics': dashboard_data.get('performance_status', {}),
            'test_results': self.test_results,
            'alerts_generated': len(dashboard_data.get('recent_alerts', [])),
            'system_health': dashboard_data.get('system_health', {}),
            'recommendations': self._generate_test_recommendations(dashboard_data)
        }

        # Save detailed report
        report_file = Path(f"comprehensive_system_test_report_{int(time.time())}.json")
        with open(report_file, 'w') as f:
            import json
            json.dump(report, f, indent=2, default=str)

        print(f"ðŸ“ Detailed report saved to: {report_file}")
        return report

    def _generate_test_recommendations(self, dashboard_data: Dict[str, Any]) -> List[str]:
        """Generate test recommendations based on results."""
        recommendations = []

        # Check system integration health
        integration = dashboard_data.get('system_integration', {})

        # Behavior trees
        bt_health = integration.get('behavior_trees', {}).get('health_percentage', 0)
        if bt_health < 80:
            recommendations.append("âš ï¸ Improve behavior tree reliability and monitoring")

        # State machines
        sm_transitions = integration.get('state_machines', {}).get('transition_success_rate', 0)
        if sm_transitions < 95:
            recommendations.append("âš ï¸ Review state machine transition logic and error handling")

        # Network
        net_health = integration.get('network', {}).get('network_health_score', 0)
        if net_health < 80:
            recommendations.append("âš ï¸ Improve network connectivity and failover mechanisms")

        # Performance
        perf_status = dashboard_data.get('performance_status', {})
        motion_control = perf_status.get('motion_control', {}).get('status', 'UNKNOWN')
        if motion_control != 'PASS':
            recommendations.append("âš ï¸ Address motion control performance issues")

        # Simulator
        sim_stability = integration.get('simulator', {}).get('simulation_stability', 'UNKNOWN')
        if sim_stability != 'GOOD':
            recommendations.append("âš ï¸ Improve simulator stability and real-time performance")

        # Overall health
        system_health = dashboard_data.get('system_health', {}).get('overall_health', 'UNKNOWN')
        if system_health != 'HEALTHY':
            recommendations.append("âš ï¸ Address system health issues before deployment")

        if not recommendations:
            recommendations.append("âœ… All systems performing well - ready for deployment")

        return recommendations

    def print_test_summary(self, report: Dict[str, Any]):
        """Print human-readable test summary."""
        print("\n" + "="*80)
        print("ðŸ§ª COMPREHENSIVE SYSTEM INTEGRATION TEST SUMMARY")
        print("="*80)

        metadata = report.get('test_metadata', {})
        print(".1f")
        # System integration status
        integration = report.get('system_integration', {})
        print("\nðŸ”— SYSTEM INTEGRATION STATUS:")
        print(f"  Behavior Trees: {integration.get('behavior_trees', {}).get('healthy_trees', 0)}/{integration.get('behavior_trees', {}).get('total_trees', 0)} healthy")
        print(f"  State Machines: {integration.get('state_machines', {}).get('active_machines', 0)}/{integration.get('state_machines', {}).get('total_machines', 0)} active")
        print(f"  Network: {integration.get('network', {}).get('active_connections', 0)}/{integration.get('network', {}).get('total_connections', 0)} connected")
        print(f"  Simulator: {integration.get('simulator', {}).get('active_simulations', 0)}/{integration.get('simulator', {}).get('total_simulations', 0)} running")

        # Performance status
        performance = report.get('performance_status', {})
        print("\nðŸ“Š PERFORMANCE STATUS:")
        print(f"  Motion Control: {performance.get('motion_control', {}).get('status', 'UNKNOWN')}")
        print(f"  Communication: {performance.get('communication', {}).get('status', 'UNKNOWN')}")
        print(f"  Resources: {performance.get('resources', {}).get('status', 'UNKNOWN')}")

        # System health
        health = report.get('system_health', {})
        print("\nâ¤ï¸ SYSTEM HEALTH:")
        print(f"  Overall: {health.get('overall_health', 'UNKNOWN')}")
        print(".1f")
        # Recommendations
        recommendations = report.get('recommendations', [])
        if recommendations:
            print("\nðŸ’¡ RECOMMENDATIONS:")
            for rec in recommendations:
                print(f"  {rec}")

        print("\n" + "="*80)


def run_comprehensive_system_test():
    """Run the comprehensive system integration test."""
    tester = ComprehensiveSystemTester()
    report = tester.run_comprehensive_system_test()
    tester.print_test_summary(report)

    return report


def start_dashboard_server():
    """Start the metrics dashboard server."""
    print("ðŸš€ Starting Metrics Dashboard Server...")
    print("ðŸ“Š Dashboard will be available at: http://localhost:8080")
    print("ðŸ”— Includes full system integration monitoring")
    print()

    try:
        run_server(8080)
    except KeyboardInterrupt:
        print("\nðŸ›‘ Dashboard server stopped")


def main():
    """Main entry point."""
    import argparse

    parser = argparse.ArgumentParser(description='URC 2026 Comprehensive System Integration Test')
    parser.add_argument('--test-only', action='store_true',
                       help='Run tests only (no dashboard)')
    parser.add_argument('--dashboard-only', action='store_true',
                       help='Start dashboard only (no tests)')
    parser.add_argument('--duration', type=int, default=120,
                       help='Test duration in seconds (default: 120)')

    args = parser.parse_args()

    if args.dashboard_only:
        # Start dashboard with simulated data
        from tests.performance.system_integration_monitor import system_monitor
        system_monitor.simulate_test_data()
        start_dashboard_server()

    elif args.test_only:
        # Run tests only
        tester = ComprehensiveSystemTester()
        tester.test_duration = args.duration
        report = tester.run_comprehensive_system_test()
        tester.print_test_summary(report)

    else:
        # Run tests and start dashboard
        print("Running comprehensive system tests with dashboard...")
        print("Dashboard will start after test completion")
        print()

        tester = ComprehensiveSystemTester()
        tester.test_duration = args.duration
        report = tester.run_comprehensive_system_test()
        tester.print_test_summary(report)

        print("\n" + "="*50)
        print("ðŸŽ¯ STARTING METRICS DASHBOARD")
        print("="*50)

        # Start dashboard
        start_dashboard_server()


if __name__ == "__main__":
    main()
